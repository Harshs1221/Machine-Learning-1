{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2782ed4-56d8-44f9-b519-2ec3e3055b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is a parameter?\n",
    "\"\"\"\n",
    "Ans1. In machine learning models, parameter are the values that the algorithm learns during\n",
    "      training to make predicitions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e0134-f965-40b0-a979-c462d35f92b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.  What is correlation?\n",
    " # What does negative correlation mean?\n",
    "\"\"\"\n",
    "Ans2. Correlation is a statistical measures that quantifies the relationship between two variables.\n",
    "      It tell us whether and how strongly two variables are related.\n",
    "      Correlation range from -1 to +1.\n",
    "      . Negative correlation means when one variable increases, the other decreases proportionally or vice-versa.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb9dc7-4d73-4a0f-b2ef-c92fa1b338fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. Define Machine Learning. What are the main components in Machine Learning?\n",
    "\"\"\"\n",
    "Ans3. Machine learning learn pattern from the data through algorithms and statistical methods.\n",
    "\n",
    "        Main components in Machine leaarning are:\n",
    "        1. Data:\n",
    "            The raw information we use to train the model. It is like a ingredients of a recipe.\n",
    "        2. Features:\n",
    "            Specific parts of the dara that are important for making predictions.\n",
    "        3. Model:\n",
    "            The machine learning system that learns pattern from the data.\n",
    "        4. Algorithm:\n",
    "            The step-by-step process the model follow to learn. \n",
    "        5. Training:\n",
    "            Feeding data to the model so it can learn.\n",
    "        6. Testing:\n",
    "            Checking how well the model works on new data.\n",
    "        7. Evaluation Metrics:\n",
    "            Tools to measure how well the model is performing.\n",
    "                \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234348a-a698-4636-a6ee-83c2917fa689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How does loss value help in determining whether the model is good or not?\n",
    "\"\"\"\n",
    "Ans4. The loss value plays a key role in determining how well a machine learning model is performing.\n",
    "      It measure the difference between the model's prediction and the actual target values.\n",
    "      Essentially, the loss is a numerical value that quantifies the model's error.\n",
    "      Here, why it is important:\n",
    "\n",
    "        1. Indicator of Model Accuracy:\n",
    "                A loss value indicates that the model's prediction are close to the actual values,\n",
    "                which generally means the model is performing well.\n",
    "                A high loss value shows the model predictiona are far from the actual targets,\n",
    "                singalling that the model needs improvement.\n",
    "        \n",
    "        2. Optimiztion During Training:\n",
    "                During training, the loss value is minimized using optimization algorithms like\n",
    "                gradient descent.\n",
    "                The model update its parameter to reduce the loss and improve its predictive \n",
    "                performance.\n",
    "                \n",
    "        3. Helps Iddentify Overfitting and Underfitting:\n",
    "               . If the loss is low on training data but high on the testing data, the model \n",
    "                 might be Overfitting.\n",
    "               . If the loss if high on both training and testing data, the model might be \n",
    "                 underfitting.\n",
    "                 \n",
    "        4. Comparing Different Models:\n",
    "                Loss value can be used to compare different models or algorithms.\n",
    "                \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75f308-edb3-4677-bb59-bf4d638e2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. What are continuous and categorical variables?\n",
    "\"\"\"\n",
    "Ans5. Continous variable:\n",
    "        . These are variables that can take on an infinite number of value within a range.\n",
    "          Example :\n",
    "                  height 150cm. 160.3cm, time 12.3 seconds etc\n",
    "\n",
    "      Categorical Variables:\n",
    "        . These are variables that represent distinct groups or categories.\n",
    "        . They can be either nominal or ordinal.\n",
    "        . Example: rating(poor, average, good), gender(male, female)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c11a2-983b-4d3a-bd3e-3655ae243214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6.  How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "\"\"\"\n",
    "Ans6. Handling categorical variables effectively is crucial in machine learning, as many algorithms\n",
    "      work better with numerical data.\n",
    "      The common tecnhiques are:\n",
    "\n",
    "      1. One-Hot Encoding:\n",
    "              . Convert each category into a separate binary column(0 or 1).\n",
    "              . Useful when categoris have no inherent order.\n",
    "              . like color:[Red, blue, Green] --> [1,0,0], [0,1,0], [0,0,1]\n",
    "      \n",
    "      2. Label Encoding:\n",
    "              . Assigns a unique integer to each catgory.\n",
    "              . usefull for ordinal data where the order matters.\n",
    "              . like size[small, medium, large] --> [0,1,2]\n",
    "      \n",
    "      3. Frequency Encoding:\n",
    "              . Replaces categories with their frequency in th dataset.\n",
    "              . like city: [delh, mumbai, mumbai] --> [1, 2, 2]\n",
    "\n",
    "      4. Target Encodig:\n",
    "              . Replaces categories with the mean of the target variable for each category.\n",
    "              . like if target = [1(pass), 0(fail) and subject =[math,english]].\n",
    "      \n",
    "      5. Embedding:\n",
    "              . Represents categories as dense vectors of real numbers.\n",
    "              . Useful for handling a larg number of catgories. (e.g., words in  natural language).\n",
    "\n",
    "      6. Leave-One-Out Encoding::\n",
    "              . A variation of target encoding where the mean of the target is calculated\n",
    "                excluding the current row, avoiding data leakage.\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee9fb2-e646-4709-be9c-0e64b6f95094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. What do you mean by training and testing a dataset?\n",
    "\"\"\"\n",
    "Ans7. Training and testing datasets are essential concepts in machine learning to evaluate\n",
    "      how well a model learns pattern and performs on new data.\n",
    "\n",
    "      1. Training Datasets:\n",
    "            . The training dataset is primirily used to train the model, but during the\n",
    "              process, we can also perform validation to fine-tune model aand assess\n",
    "              how well its learning.\n",
    "            . Thinking of it as a studdent practising math problems repeatedly to understand\n",
    "              concepts.\n",
    "\n",
    "      2. Testing Dataset:\n",
    "            . This is a separate portion of data, not shown to the model during training.\n",
    "            . The goal is to evaluate the model performance on unseen data.\n",
    "            . It like taking the surprise teest to see  how well student understand the\n",
    "              concept.\n",
    "            \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee429f-56a1-48e9-92ec-a4b5e60ba688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What is sklearn.preprocessing?\n",
    "\"\"\"\n",
    "Ans8. sklearn.preprocessing is a module in the Scikitlearn Library used for preprocessing data \n",
    "      before feeding it into a machine learning model.\n",
    "      Preprocessing is a essential step in machine learning as it ensures the data in the best \n",
    "      format and scales for models to achieve optimal performance.\n",
    "\n",
    "      sklearn.preprocessing module provides:\n",
    "      \n",
    "      . transform categorical variables into numerical ones.\n",
    "      . Standardize or normalize data for better model performance.\n",
    "      . Handle missing value and prepares the data for machine learning algorithms.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614ed26-5ed4-4b28-8ebd-e141c24675a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9.  What is a Test set?\n",
    "\"\"\"\n",
    "Ans9. Test set is a portion of the dataset that  is used to evaluate the final performance of a\n",
    "      machine learning model.\n",
    "      Test set is used after the traning and validation processes are completed.\n",
    "      It checks whether the model can make accurate prediction on new, unseen data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719e8b3-edd8-4260-87bf-0ba0751ee133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10. How do we split data for model fitting (training and testing) in Python?\n",
    "# How do you approach a Machine Learning problem?\n",
    "\"\"\"\n",
    "Ans10. For model fitting you can split the data in 80:20 ratio.\n",
    "        \n",
    "        . 80% data is used for training the model. Training is primirily used for train the \n",
    "          model  but during the process, we can also performm validation for fine-tune model\n",
    "          and assess how well its learning.\n",
    "        \n",
    "        . 20% data is used for testing the model. It is used separate portion of data which is \n",
    "          not shown in model during training. The goal of testing is to evaluate the performance\n",
    "          of th model on unseen data.\n",
    "\n",
    "       How to approch a machine learning problem:\n",
    "\n",
    "        1. Understand the Problem:\n",
    "                . Define the problem you are trying to solve.\n",
    "                . Identify the desired outcomes and evaluates the feasibility of solving it using \n",
    "                  machine learning.\n",
    "                  \n",
    "        2. Collect and Explore the dataa:\n",
    "                . Gather relevant datasets.\n",
    "                . Peform EDA to understand the data structure, distribution and potiential issue.\n",
    "                  (like missing value and outliers.)\n",
    "        \n",
    "        3. Preprocess and Clean data:\n",
    "                . Handdling Missing values and  outliers.\n",
    "                . Normalize, scale or transform feature as needed.\n",
    "       \n",
    "        4. Feature Engineering\n",
    "                . Create and select the relevant feature to reduce complexity and improve efficiency.\n",
    "        \n",
    "        5. Select model\n",
    "                . Choose machine learning algorithm that suits your model(e.g., linear regression,\n",
    "                  decision tree etc.)\n",
    "        \n",
    "        6. Split Data:\n",
    "                . Divide the dataset into training, validation and test sets to avoid overfitting\n",
    "                  and underfitting.\n",
    "        \n",
    "        7. Train the model:\n",
    "                . Train your model on the training dataset and optimize its parameter.\n",
    "        \n",
    "        8. Evaluate the model:\n",
    "                . Evaluate the performance of the model using validation and test set.\n",
    "\n",
    "        9. Deployement:\n",
    "                . If satisfied with the performance, deploy the model into production\n",
    "                  environment.\n",
    "                .\n",
    "        \n",
    "            \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a3cc79-b927-48d0-8b48-0f67fcd5bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q11. Why do we have to perform EDA before fitting a model to the data?\n",
    "\"\"\"\n",
    "Ans11. Exploratory data analysis is critical for understanding and preparing your data.\n",
    "\n",
    "        1. Understand Data Distribution:\n",
    "            . Grasp patterns, trends and relationship between varriables. \n",
    "\n",
    "        2. Detect Issue:\n",
    "            . Idenntify missig values, outlier or incorrect  data that could negatively\n",
    "              impact the model performance.\n",
    "\n",
    "        3. Feture Selection and Engineering:\n",
    "            . Determine the relevant feature and explore oppurtunites for creating new \n",
    "              ones.\n",
    "\n",
    "        4. Check Assumption:\n",
    "            . Some model have assumption about the data(ee.g., linearity, normal distribution).\n",
    "              Through EDA, you can check whether your data aligns with assumption or require \n",
    "              any transformation.\n",
    "\n",
    "        5. Uncover Insights:\n",
    "            . Reveal hidden insights that can shape the problem solving strategy. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80db796-3323-434b-9555-dbb163c4d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q12. What is correlation?\n",
    "\"\"\"\n",
    "Ans12. Correlation is a statistical measures that quantifies the relationship between two variables.\n",
    "       It tell us whether and how strongly two variables are related.\n",
    "       Correlation range from -1 to +1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30077c2c-13d9-471d-bbc6-b06674c2127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q13. What does negative correlation mean?\n",
    "\"\"\"\n",
    "Ans13. Negative correlation means when one variable increases, the other decreases proportionally and vice-versa.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc23ee-d8bf-4d05-95da-6a782451cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q14. How can you find correlation between variables in Python?\n",
    "\"\"\"\n",
    "Ans14. To find the correlation between variables in python you can use Numpy, pandas or Seaborn.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bec6fd-9cf6-4f0c-86da-aab3bd3fdc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas\n",
    "#corr() is used to find correlation between variables using pandas\n",
    "corr_1 = data.corr()\n",
    "\n",
    "# corelation btween two ccolumn\n",
    "correlation = data['column1'].corr(data['column2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0d84ef1-0090-4448-91eb-3e2461215c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy\n",
    "correltion = np.corrcoef('column1', 'column2')   # column1 and 2 are arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da295c39-d26e-4f43-8b96-0abadd6aff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seaborn is for visulaization\n",
    "# use heatmap for understanding correlation\n",
    "\n",
    "sns.heatmap(data.corr(), annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a7083-f8ec-4ec8-bf46-c29f895ebbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q15. What is causation? Explain difference between correlation and causation with an example.\n",
    "\"\"\"\n",
    "Ans15. Causation means that one event directly causes another event to happen. There is cause-and-effect\n",
    "       relationship betwen two variables.\n",
    "       For example: if you turn on a light switch, the light turns on- the switch causes the light\\\n",
    "                    to illuminate.\n",
    "\n",
    "        Difference between Correlation and causation:\n",
    "\n",
    "        1. Correlation:\n",
    "            . Correlation quantifies the relationship between two variables.\n",
    "            . It only shows relationship or pattern. It dos not imply that one causes the other.\n",
    "            . Example: There might be relationship between icecream sales and number of people\n",
    "                        swimming. When icecream sales increases, more people go swimming.\n",
    "                        But eating ice-crem does not cause people to swim - both are relatd to\n",
    "                        summer weather.\n",
    "        2. Causation\n",
    "            . Causation establishes that one variable is the reason for the change in other variable.\n",
    "            . like cutting trees affects global warming.\n",
    "            . drinking too much alcohol causes intoxication.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f754e-e3c9-4d94-9422-1c21f6f72248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q16. What is an Optimizer? What are different types of optimizers? Explain each with an example. \n",
    "\"\"\"\n",
    "Ans16. An optimizer in machine learning is an algorithm or method used to adjust the weights and biases\n",
    "       of a model to minimize the loss (error) during training.\n",
    "       . The goal of optimizer is to make the model as accurate as possible by finiding the optimal\n",
    "         values for its parameter.\n",
    "\n",
    "        Types of optimizers:\n",
    "            1. Gradient Descent: \n",
    "                        Moves the model step by step in the direction of reducing error. Simple but\n",
    "                        may be slow.\n",
    "                        Example: Adjusting weights for better prediction.\n",
    "            2. Momentum:\n",
    "                        Speeds up Gradient Descent by adding \"momentum\", so it doesn't get stuck in\n",
    "                        flat areas.\n",
    "                        example: A ball rolling down a hill faster.\n",
    "            3. AdaGrad:\n",
    "                        Adjust the step size (learning rate) for each parameter, giving smaller\n",
    "                        steps to frequently occuring features.\n",
    "                        example: great for data with rare features.\n",
    "            4. RMSProp:\n",
    "                        Dynamically changes the step size to handle problems where data patterns\n",
    "                        keep changing (non-stationary).\n",
    "                        Example: Works well for sequence-based tasks like time-series\n",
    "            5. Adam:\n",
    "                        Combines the best of Momentum and RMSProp for faster and efficient\n",
    "                        training.\n",
    "                        Example: Most popular for a wide range of tasks \n",
    "            \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baeb028-38e9-4eaf-b83d-50654bc54475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q17. What is sklearn.linear_model ?\n",
    "\"\"\"\n",
    "Anns17. sklearn.linear_model is a module in the Scikit-learn library that contains implementations of various\n",
    "        linear models for regression and classification tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed824865-240c-4acb-9288-efc88ac6e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q18. What does model.fit() do? What arguments must be given?\n",
    "\"\"\"\n",
    "Ans18. model.fit() is used to train machine learning model or preprocessing model.\n",
    "\n",
    "        1. For machine learning model:\n",
    "                . If you are using supervised model(like regression or random forest), fit() makes the\n",
    "                  model to learn relationship between x variable and target variable (y).\n",
    "\n",
    "        2. For preprocessing:\n",
    "                . In preprocessing, fit() calculates the statistical method like mean and standard \n",
    "                  deviation of your data, which are later used for scaler and normalize it. \n",
    "\n",
    "        Arguments for model.fit():\n",
    "\n",
    "            1. For machine learning:\n",
    "                    x feature variable\n",
    "                    y target variable\n",
    "\n",
    "            2. For preprocessing:\n",
    "                    x feature matrix\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b177ff9-e428-4b23-9ebb-f85488497c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q19. What does model.predict() do? What arguments must be given?\n",
    "\"\"\"\n",
    "Ans19. The model.predict() method is used in machine learning to make prediction on new data\n",
    "       based on a trained model.\n",
    "       . It takes input data and outputs the predictd values.\n",
    "       . It predicts numerical values (e.g., house prices, stock values)\n",
    "       . It predicts the class label for the given input data.\n",
    "\n",
    "       Arguments for model.prdict():\n",
    "       The main argument for model.predict is the input data. The input data has some format:\n",
    "\n",
    "       1. Input data:\n",
    "               The data you provide should look like the data the model was trained on.\n",
    "               for example the column are same while train your model and in input data.\n",
    "       2. Same Shape:\n",
    "               The number of column in your new data should match the columns in the data \n",
    "               used for training.\n",
    "       3. Preprocessed Data:\n",
    "               If you did scaling and encoding during training, you must do the same thing\n",
    "               to your new data beffore giving it to the model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6cf2ea-57bb-42cc-bab0-3b06f50a13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q20. What are continuous and categorical variables?\n",
    "\"\"\"\n",
    "Ans20. Continous variable:\n",
    "        . These are variables that can take on an infinite number of value within a range.\n",
    "          Example :\n",
    "                  height 150cm. 160.3cm, time 12.3 seconds etc\n",
    "\n",
    "      Categorical Variables:\n",
    "        . These are variables that represent distinct groups or categories.\n",
    "        . They can be either nominal or ordinal.\n",
    "        . Example: rating(poor, average, good), gender(male, female)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a61b4-538e-4f39-8b09-5ab7d480c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q21. What is feature scaling? How does it help in Machine Learning?\n",
    "\"\"\"\n",
    "Ans21. Feature scaling trying to bring all the feature of the dataset to the same scale.\n",
    "\n",
    "       How does it help  in Machine learning:\n",
    "\n",
    "        1. Improves Model Performance:\n",
    "                . Some algorithms like Vector Machines, Gradied based methods etc, rely heavily on\n",
    "                  distance calculations . feature scalinng ensures fair contribution from all\n",
    "                  features.\n",
    "        2. Optimization becomes faster:\n",
    "                . Scaling helps optimization algorithms work efficiently by making sure that they don't\n",
    "                  get stuck handling large differences in fature sizes.\n",
    "                . This speed up the training proccess and smoothens the learning.      \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179983f9-1d2c-4fad-b815-ed5b323a556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q22. How do we perform scaling in Python? \n",
    "\"\"\"\n",
    "Ans22. \n",
    "        1. Standardization:\n",
    "                . For Standardization, import standardScaler from skleran.preprocessing.\n",
    "                . this method scale the data to have a mean = 0 and standard deviation = 1\n",
    "        \n",
    "        2. Normalization\n",
    "                . For normalization, import MinMaxScaler from sklearn.preprocessing.\n",
    "                . Scales data to a fixed range [0,1]\n",
    "\n",
    "        3. Unit vector\n",
    "                . For unit Vector, imporrt normalize from sklearn.preprocessing\n",
    "                . Scale the data such that each row or sample has a unit form.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29943262-b877-4b3f-a04c-efa41d251af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q23. What is sklearn.preprocessing?\n",
    "\"\"\"\n",
    "Ans8. sklearn.preprocessing is a module in the Scikitlearn Library used for preprocessing data \n",
    "      before feeding it into a machine learning model.\n",
    "      Preprocessing is a essential step in machine learning as it ensures the data in the best \n",
    "      format and scales for models to achieve optimal performance.\n",
    "\n",
    "      sklearn.preprocessing module provides:\n",
    "      \n",
    "      . transform categorical variables into numerical ones.\n",
    "      . Standardize or normalize data for better model performance.\n",
    "      . Handle missing value and prepares the data for machine learning algorithms.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3986a60-13af-42a0-a865-a4969023efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q24. How do we split data for model fitting (training and testing) in Python?\n",
    "\"\"\"\n",
    "Ans24. For model fitting you can split the data in 80:20 ratio.\n",
    "\n",
    "        . Training\n",
    "          80% data is used for training the model. Training is primirily used for train the \n",
    "          model  but during the process, we can also performm validation for fine-tune model\n",
    "          and assess how well its learning.\n",
    "\n",
    "        . Testing\n",
    "          20% data is used for testing the model. It is used separate portion of data which is \n",
    "          not shown in model during training. The goal of testing is to evaluate the performance\n",
    "          of th model on unseen data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b01c6-c636-45ed-b79f-2c65d4288678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q25. Explain data encoding?\n",
    "\"\"\"\n",
    "Ans25 Data encoding converts categorical data into a numerical data, allowing machine learning models\n",
    "      to process it effectively.\n",
    "\n",
    "      Types of Data encoding are:\n",
    "          1. Nominal/One Hot Encoding:\n",
    "                      . This techniques create binary column for categorical column.\n",
    "                      . No order in the data\n",
    "                      . like red = 0,0,1 , blue = 1,0,0 , green= 0,1,0\n",
    "          2. Label and Ordinal\n",
    "                      . Label encoding convert each category into a unique numerical value.\n",
    "                      . like red -> 1, green -> 2, blue -> 3\n",
    "                      . It assumes some form of order, which is suitable for ordinal data.\n",
    "          3. Target guided Ordinal Encoding\n",
    "                      . Encodes categories based on their relationship with the target variable.\n",
    "                      . Usefull when we have large number of unique categories in categorical data.\n",
    "                      . It replaces with mean and median of corresponding arget variables.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec98d5-9646-4a42-a969-c65bfb9af519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b482d4-0d41-4c13-b9df-d5e70cf08dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5828e4-e54c-423e-9fb5-83417264f093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f98ec4-2cf7-40b0-9af3-84c85c949486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abf7ff-b256-48fe-abad-6089cf86ef88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
